{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPPSKttxBTtXw3a3RTXVsgm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51146eb987d14cd0bac9301e26b3beae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7abc24cd773b402f9674adac8c4cca2e",
              "IPY_MODEL_a4f2456701954c89afc38734b77acff0",
              "IPY_MODEL_378959849a6d48c6ae296e8bebeb42e1"
            ],
            "layout": "IPY_MODEL_93e5fa59aebb4c7a865722c5cb723b33"
          }
        },
        "7abc24cd773b402f9674adac8c4cca2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ec7a5394b934bfd8a9b146583af080a",
            "placeholder": "​",
            "style": "IPY_MODEL_5c855c92fa7042359563034e805d2f95",
            "value": "  0%"
          }
        },
        "a4f2456701954c89afc38734b77acff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_549860d6071c482ea43b8f1daab810f3",
            "max": 120000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd5e50580c0b4c63b8e41f263765c9a0",
            "value": 533
          }
        },
        "378959849a6d48c6ae296e8bebeb42e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3669c65d683a46458989e237b0e4dba4",
            "placeholder": "​",
            "style": "IPY_MODEL_3bd12208a06448d4b13f436d667660b2",
            "value": " 533/120000 [03:43&lt;13:56:06,  2.38it/s]"
          }
        },
        "93e5fa59aebb4c7a865722c5cb723b33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ec7a5394b934bfd8a9b146583af080a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c855c92fa7042359563034e805d2f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "549860d6071c482ea43b8f1daab810f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd5e50580c0b4c63b8e41f263765c9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3669c65d683a46458989e237b0e4dba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bd12208a06448d4b13f436d667660b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keenanpepper/self-ablating-transformers/blob/main/Self_ablating_transformer_with_smooth_top_K.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "UdoAI2ygESas"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZiyVoPQD8g3",
        "outputId": "7e2bfda5-2946-4af3-b8de-52ec34f03999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'self-ablating-transformers'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 74 (delta 7), reused 14 (delta 4), pack-reused 42 (from 1)\u001b[K\n",
            "Receiving objects: 100% (74/74), 57.09 MiB | 12.79 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/keenanpepper/self-ablating-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"keenanpepper@gmail.com\"\n",
        "!git config --global user.name \"Keenan Pepper\""
      ],
      "metadata": {
        "id": "lWLMtMsmHE5L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd self-ablating-transformers/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1af5sIaEMV1",
        "outputId": "6e51bea7-6ece-4562-ba52-bd54e3045bdc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/self-ablating-transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxpPa5uwEXn9",
        "outputId": "857a4860-feda-4711-cc01-f7c61ea04fe6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting circuitsvis==1.41.0 (from -r requirements.txt (line 1))\n",
            "  Downloading circuitsvis-1.41.0-py3-none-any.whl.metadata (995 bytes)\n",
            "Collecting datasets==2.21.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting matplotlib==3.9.2 (from -r requirements.txt (line 3))\n",
            "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.26.4)\n",
            "Collecting seaborn==0.13.2 (from -r requirements.txt (line 5))\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting tiktoken==0.7.0 (from -r requirements.txt (line 6))\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.4.0+cu121)\n",
            "Requirement already satisfied: tqdm==4.66.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.66.5)\n",
            "Collecting transformer-lens==2.4.1 (from -r requirements.txt (line 9))\n",
            "  Downloading transformer_lens-2.4.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: transformers==4.44.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.44.2)\n",
            "Collecting importlib-metadata<6.0.0,>=5.1.0 (from circuitsvis==1.41.0->-r requirements.txt (line 1))\n",
            "  Downloading importlib_metadata-5.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (3.16.0)\n",
            "Collecting pyarrow>=15.0.0 (from datasets==2.21.0->-r requirements.txt (line 2))\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.21.0->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Collecting xxhash (from datasets==2.21.0->-r requirements.txt (line 2))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==2.21.0->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==2.21.0->-r requirements.txt (line 2)) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 3)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 3)) (1.4.7)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.7.0->-r requirements.txt (line 6)) (2024.5.15)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==2.4.1->-r requirements.txt (line 9)) (0.33.0)\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer-lens==2.4.1->-r requirements.txt (line 9))\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting better-abc<0.0.4,>=0.0.3 (from transformer-lens==2.4.1->-r requirements.txt (line 9))\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==2.4.1->-r requirements.txt (line 9)) (0.8.0)\n",
            "Collecting fancy-einsum>=0.0.3 (from transformer-lens==2.4.1->-r requirements.txt (line 9))\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer-lens==2.4.1->-r requirements.txt (line 9))\n",
            "  Downloading jaxtyping-0.2.34-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==2.4.1->-r requirements.txt (line 9)) (13.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer-lens==2.4.1->-r requirements.txt (line 9)) (0.1.99)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer-lens==2.4.1->-r requirements.txt (line 9)) (4.12.2)\n",
            "Collecting wandb>=0.13.5 (from transformer-lens==2.4.1->-r requirements.txt (line 9))\n",
            "  Downloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2->-r requirements.txt (line 10)) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2->-r requirements.txt (line 10)) (0.19.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->-r requirements.txt (line 7)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->-r requirements.txt (line 7)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->-r requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==2.4.1->-r requirements.txt (line 9)) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis==1.41.0->-r requirements.txt (line 1)) (3.20.1)\n",
            "Collecting typeguard==2.13.3 (from jaxtyping>=0.2.11->transformer-lens==2.4.1->-r requirements.txt (line 9))\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.21.0->-r requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.21.0->-r requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.9.2->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.21.0->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.21.0->-r requirements.txt (line 2)) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.21.0->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.21.0->-r requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==2.4.1->-r requirements.txt (line 9)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==2.4.1->-r requirements.txt (line 9)) (2.16.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==2.4.1->-r requirements.txt (line 9)) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer-lens==2.4.1->-r requirements.txt (line 9))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens==2.4.1->-r requirements.txt (line 9))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==2.4.1->-r requirements.txt (line 9)) (4.3.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==2.4.1->-r requirements.txt (line 9)) (3.20.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer-lens==2.4.1->-r requirements.txt (line 9))\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb>=0.13.5->transformer-lens==2.4.1->-r requirements.txt (line 9))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==2.4.1->-r requirements.txt (line 9)) (71.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->-r requirements.txt (line 7)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.4.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==2.4.1->-r requirements.txt (line 9))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==2.4.1->-r requirements.txt (line 9)) (0.1.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==2.4.1->-r requirements.txt (line 9))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading circuitsvis-1.41.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformer_lens-2.4.1-py3-none-any.whl (174 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.8/174.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
            "Downloading jaxtyping-0.2.34-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: better-abc, xxhash, typeguard, smmap, setproctitle, sentry-sdk, pyarrow, importlib-metadata, fancy-einsum, docker-pycreds, dill, beartype, tiktoken, multiprocess, matplotlib, jaxtyping, gitdb, seaborn, gitpython, circuitsvis, wandb, datasets, transformer-lens\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.4.0\n",
            "    Uninstalling importlib_metadata-8.4.0:\n",
            "      Successfully uninstalled importlib_metadata-8.4.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.1\n",
            "    Uninstalling seaborn-0.13.1:\n",
            "      Successfully uninstalled seaborn-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "inflect 7.3.1 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beartype-0.14.1 better-abc-0.0.3 circuitsvis-1.41.0 datasets-2.21.0 dill-0.3.8 docker-pycreds-0.4.0 fancy-einsum-0.0.3 gitdb-4.0.11 gitpython-3.1.43 importlib-metadata-5.2.0 jaxtyping-0.2.34 matplotlib-3.9.2 multiprocess-0.70.16 pyarrow-17.0.0 seaborn-0.13.2 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 tiktoken-0.7.0 transformer-lens-2.4.1 typeguard-2.13.3 wandb-0.18.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import datasets\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "F2JS9HclFrfe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.gpt_neo import GPTNeoWithSelfAblation, soft_top_k\n",
        "from model.config import GPTNeoWithSelfAblationConfig"
      ],
      "metadata": {
        "id": "qc81QHUdF5FW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTNeoWithSelfAblation(GPTNeoWithSelfAblationConfig(hidden_size=128))"
      ],
      "metadata": {
        "id": "Qn5AAg-cGAyh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW-XjtbDGRl0",
        "outputId": "ec2137dc-9db1-4bab-9375-90387b29359c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoWithSelfAblation(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 128)\n",
              "    (wpe): Embedding(2048, 128)\n",
              "    (h): ModuleList(\n",
              "      (0-7): 8 x GPTNeoBlockWithSelfAblation(\n",
              "        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): AttentionWithSelfAblation(\n",
              "          (attention): ModuleDict(\n",
              "            (k_proj): Linear(in_features=128, out_features=128, bias=False)\n",
              "            (v_proj): Linear(in_features=128, out_features=128, bias=False)\n",
              "            (q_proj): Linear(in_features=128, out_features=128, bias=False)\n",
              "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLPWithSelfAblation(\n",
              "          (c_fc): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (c_proj): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=128, out_features=50257, bias=False)\n",
              "  (attention_ablations_head): Linear(in_features=128, out_features=1024, bias=True)\n",
              "  (neuron_ablations_head): Linear(in_features=128, out_features=4096, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.zeros((1,1), dtype=torch.long))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsvW43LAGWpY",
        "outputId": "6a6fed68-f84f-4160-e867-0c4dc0852c52"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'logits': tensor([[[ 0.0269, -0.3266,  0.8674,  ..., -1.3128, -0.1785,  0.3069]]],\n",
              "        grad_fn=<UnsafeViewBackward0>),\n",
              " 'logits_ablated': tensor([[[ 0.4077,  0.1292,  0.4486,  ..., -0.9105,  0.3658,  0.1320]]],\n",
              "        grad_fn=<UnsafeViewBackward0>),\n",
              " 'L_base': None,\n",
              " 'L_ablated': None,\n",
              " 'loss': None,\n",
              " 'attention_ablations': tensor([[[[0.0218, 0.0235, 0.0227,  ..., 0.1646, 0.0247, 0.0432],\n",
              "           [0.0115, 0.0421, 0.0317,  ..., 0.0584, 0.0607, 0.0813],\n",
              "           [0.0195, 0.0309, 0.0274,  ..., 0.0481, 0.0359, 0.1408],\n",
              "           ...,\n",
              "           [0.1007, 0.0483, 0.1678,  ..., 0.0824, 0.0918, 0.0668],\n",
              "           [0.0808, 0.0720, 0.0440,  ..., 0.0658, 0.0338, 0.0466],\n",
              "           [0.0242, 0.0059, 0.0804,  ..., 0.0087, 0.0354, 0.0556]]]],\n",
              "        grad_fn=<ViewBackward0>),\n",
              " 'neuron_ablations': tensor([[[[0.0346, 0.0414, 0.0183,  ..., 0.0068, 0.0105, 0.0047],\n",
              "           [0.0394, 0.0049, 0.0138,  ..., 0.0498, 0.0156, 0.0215],\n",
              "           [0.0260, 0.0056, 0.0282,  ..., 0.0063, 0.0049, 0.0032],\n",
              "           ...,\n",
              "           [0.0046, 0.0062, 0.0080,  ..., 0.0083, 0.0396, 0.0031],\n",
              "           [0.0139, 0.0124, 0.0148,  ..., 0.0189, 0.0102, 0.0095],\n",
              "           [0.0082, 0.0122, 0.0108,  ..., 0.0187, 0.0374, 0.0073]]]],\n",
              "        grad_fn=<ViewBackward0>),\n",
              " 'attention_ablation_mask_density': None,\n",
              " 'neuron_ablation_mask_density': None}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = datasets.load_dataset(\"tdooms/TinyStories\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvuxnyKFGy0I",
        "outputId": "49fefa84-4ff9-4503-8db8-aaeb38149112"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import prepare_data"
      ],
      "metadata": {
        "id": "iUwIu2nOf_uL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, tokenizer = prepare_data(separator_token=\"<|endoftext|>\")\n",
        "print(f\"Processed data shape: {data.shape}\")\n",
        "print(f\"First 10 tokens: {data[:10]}\")\n",
        "print(f\"Last 10 tokens: {data[-10:]}\")\n",
        "\n",
        "# Decode a small portion to verify\n",
        "sample = data[:10000]\n",
        "decoded = tokenizer.decode(sample.tolist())\n",
        "print(\"\\nSample decoded text:\")\n",
        "print(decoded[:5000] + \"...\")  # Print first 500 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxKw923JgMw3",
        "outputId": "801f25e1-794c-4cf7-e15c-2e24627c6951"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.bin already exists. Skipping processing.\n",
            "Processed data shape: (396967656,)\n",
            "First 10 tokens: [ 8888    11 19919   373   845  6568    13   679   373  1016]\n",
            "Last 10 tokens: [  262 25103 12023   290  8359   257 12625  8073    13 50256]\n",
            "\n",
            "Sample decoded text:\n",
            "Today, Tommy was very excited. He was going flying with his mom and dad to a special place. There was a big green flag waving in the wind when they arrived. Tommy was so happy to see it. \n",
            "\n",
            "He hopped out of the car and ran up to the flag. He couldn't believe how big it was! He wanted to reach out and touch it. His mom said he could and Tommy smiled.\n",
            "\n",
            "He waved his little arms, trying to make the flag move. But the wind was too strong. His dad said, \"Let me help you, Tommy.\" He reached out and grabbed the green flag and waved it back and forth. \n",
            "\n",
            "Now it was Tommy's turn! He clapped his hands and laughed as he tried to move the flag. He waved it for a long time until he was too tired to do it anymore. The big green flag was so exciting to Tommy!<|endoftext|>Once upon a time there was a big fat radish. It was so yummy, but it was only meant for one special bunny. But one day, a bad fox came along, and he wanted to steal the radish. He was sneaky and sly, and he came up with a plan to try.\n",
            "\n",
            "So he crept up very slowly and he grabbed the radish with his nose. The poor bunny was so sad, but he was too scared to speak. He watched the fox take off with his radish, running away as fast as he could go.\n",
            "\n",
            "But all of a sudden, a big surprise - the radish was too fat for the fox to carry! So, reluctantly, he had to drop the radish and leave it behind. The radish rolled back to the bunny, and the fox had to go away with his tail between his legs.\n",
            "\n",
            "The little bunny hopped over and hugged the radish with joy, knowing that it was safe from the fox's ploy. The fox slinked away with nothing but his shame, and the bunny happily enjoyed his radish once again.<|endoftext|>The girl was troubled. She was tired and didn't know what to do. Suddenly she remembered something that made her smile. She ran to get her special spray. \n",
            "\n",
            "The girl held the spray bottles in her hands and looked at them closely. They smelled sweet and looked like little rainbows. She took a deep breath and didn't worry about anything else.\n",
            "\n",
            "The girl started to spray the colourful liquid all around the room. The rainbows sparkled in the light and filled the room with joy. \n",
            "\n",
            "The girl laughed and said, \"That's much better!\" She felt excited and happy and like all her troubles were gone. \n",
            "\n",
            "The girl ran around spraying more until every corner of the room was filled with colourful rainbows. She felt like all her troubles were gone, and everything was right in the world.<|endoftext|>Once upon a time there was a girl named Jane. Jane was three years old and she enjoyed playing with her friends. One day, Jane was feeling very confused. She didn't know what to do with her day. So she decided to put her notebook on the table and see what she could write. She grabbed her pencil and started to draw some pictures.\n",
            "\n",
            "Jane drew happy trees, a big sun and some flowers. She was so proud of her work. She then put the notebook away in her bag. On the way home, she thought about the afternoon. She felt a lot less confused now and was excited to show her family her notebook.\n",
            "\n",
            "When Jane arrived home, she pulled out her notebook and proudly showed it to her mom. Jane's mom was very impressed with the pictures and she gave her a big hug. Jane felt really happy and no longer confused. From then on, Jane always put her notebook away safely when she had finished using it.<|endoftext|>Sara and Ben are twins. They like to play in their bedroom. They have many toys and books and dolls. But sometimes they fight over their things. One day, Sara wants to play with Ben's truck. Ben says no. He says it is his truck and Sara has her own toys. Sara gets angry. She grabs the truck and runs away. Ben chases her and tries to get it back. They push and pull and yell.\n",
            "\n",
            "Mom hears the noise. She comes to their bedroom. She sees the mess and the tears. She is not happy. She asks them what is wrong. Sara and Ben both talk at the same time. They tell Mom that the other one is bad and mean and took their toy. Mom shakes her head. She tells them to stop and listen. She says they need to explain why they are upset and how they feel. She says they need to share and be kind and say sorry.\n",
            "\n",
            "Sara and Ben are quiet. They think about what Mom says. They look at each other. They feel sad and sorry. They both say sorry to Mom and to each other. They hug and make up. They decide to play with the truck together. They take turns and help each other. Mom smiles and hugs them. She says she is proud of them. She says they are good twins. She says she loves them. Sara and Ben say they love Mom too. They are happy. They play in their bedroom. They have fun.<|endoftext|>Once upon a time, there was a little girl named Lily. She loved to play in her garden. One day, she was playing with her toys when her friend, Timmy, came to visit. \n",
            "\n",
            "\"Hi Lily, can I play with you?\" Timmy asked. \n",
            "\n",
            "\"Sure, let's play together!\" Lily replied happily. \n",
            "\n",
            "They played hide-and-seek and tag, but then Lily heard something. \"Listen, Timmy. Do you hear that?\" s...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_data(split=\"validation\", output_file=\"validation.bin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lywtZJJ2gaHB",
        "outputId": "6b49f9d0-225b-4ff3-cca9-0bea91029723"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation.bin already exists. Skipping processing.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(memmap([ 7454,  2402,   257, ..., 20567,    13, 50256], dtype=uint16),\n",
              " <Encoding 'gpt2'>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingConfig:\n",
        "    train_file = \"train.bin\"\n",
        "    val_file = \"validation.bin\"\n",
        "    block_size = 256\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    num_batches = 120000\n",
        "    batch_size = 64\n",
        "    learning_rate = 4e-3\n",
        "    weight_decay = 0.0\n",
        "    max_grad_norm = 1.0\n",
        "    save_path = \"best_model.pt\"\n",
        "    eval_iters = 100\n",
        "    log_interval = 1000"
      ],
      "metadata": {
        "id": "BcHYPm5xjLGc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchGenerator:\n",
        "    def __init__(self, data_file, block_size, batch_size, device):\n",
        "        self.data_file = data_file\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.device_type = 'cuda' if 'cuda' in device.type else 'cpu'\n",
        "\n",
        "    def get_batch(self, shifted=True):\n",
        "        # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "        # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "        data = np.memmap(self.data_file, dtype=np.uint16, mode='r')\n",
        "\n",
        "        # Generate random starting indices\n",
        "        ix = torch.randint(len(data) - self.block_size, (self.batch_size,))\n",
        "\n",
        "        shift = 1 if shifted else 0\n",
        "        # Create input and target tensors\n",
        "        x = torch.stack([torch.from_numpy((data[i:i+self.block_size]).astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy((data[i+shift:i+shift+self.block_size]).astype(np.int64)) for i in ix])\n",
        "\n",
        "        if self.device_type == 'cuda':\n",
        "            # Pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "            x, y = x.pin_memory().to(self.device, non_blocking=True), y.pin_memory().to(self.device, non_blocking=True)\n",
        "        else:\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "D3CfNOsTjbLu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossEstimator:\n",
        "    def __init__(self, model, train_batch_gen, val_batch_gen, eval_iters):\n",
        "        self.model = model\n",
        "        self.train_batch_gen = train_batch_gen\n",
        "        self.val_batch_gen = val_batch_gen\n",
        "        self.eval_iters = eval_iters\n",
        "\n",
        "    def estimate_loss(self):\n",
        "        stat_names = [\"loss\", \"L_base\", \"L_ablated\", \"attention_ablation_mask_density\", \"neuron_ablation_mask_density\"]\n",
        "        out = {}\n",
        "        for stat in stat_names:\n",
        "            out[stat] = {}\n",
        "        self.model.eval()\n",
        "        with torch.inference_mode():\n",
        "            for split, batch_gen in [('train', self.train_batch_gen), ('val', self.val_batch_gen)]:\n",
        "                stats = {}\n",
        "                for stat in stat_names:\n",
        "                    stats[stat] = torch.zeros(self.eval_iters)\n",
        "                for k in tqdm(range(self.eval_iters)):\n",
        "                    X, Y = batch_gen.get_batch()\n",
        "                    ret = self.model(X, Y)\n",
        "                    for stat in stat_names:\n",
        "                        stats[stat][k] = ret[stat].item()\n",
        "                for stat in stat_names:\n",
        "                    out[stat][split] = stats[stat].mean()\n",
        "        self.model.train()\n",
        "        return out\n",
        "\n",
        "    def estimate_loss_pretrainedformat(self):\n",
        "        out = {}\n",
        "        self.model.eval()\n",
        "        with torch.inference_mode():\n",
        "            for split, batch_gen in [('train', self.train_batch_gen), ('val', self.val_batch_gen)]:\n",
        "                losses = torch.zeros(self.eval_iters)\n",
        "                for k in tqdm(range(self.eval_iters)):\n",
        "                    X, Y = batch_gen.get_batch(shifted=False)\n",
        "                    ret = self.model(X, labels=Y)\n",
        "                    losses[k] = ret[\"loss\"].item()\n",
        "                out[split] = losses.mean()\n",
        "        self.model.train()\n",
        "        return out"
      ],
      "metadata": {
        "id": "GCqzb25hjbtu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gptneo(model, config):\n",
        "    train_batch_gen = BatchGenerator(config.train_file, config.block_size, config.batch_size, config.device)\n",
        "    val_batch_gen = BatchGenerator(config.val_file, config.block_size, config.batch_size, config.device)\n",
        "    loss_estimator = LossEstimator(model, train_batch_gen, val_batch_gen, config.eval_iters)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_batches)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for iteration in tqdm(range(config.num_batches)):\n",
        "        model.train()\n",
        "\n",
        "        # Get batch\n",
        "        x, y = train_batch_gen.get_batch()\n",
        "\n",
        "        # Forward pass\n",
        "        loss = model(x, targets=y)[\"loss\"]\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        if config.max_grad_norm:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Logging\n",
        "        if (iteration + 1) % config.log_interval == 0:\n",
        "            stats = loss_estimator.estimate_loss()\n",
        "            print(f\"Iteration {iteration}: train loss {stats['loss']['train']:.4f}, val loss {stats['loss']['val']:.4f}\")\n",
        "            print(f\"train L_base {stats['L_base']['train']:.4f}, val L_base {stats['L_base']['val']:.4f}\")\n",
        "            print(f\"train L_ablated {stats['L_ablated']['train']:.4f}, val L_ablated {stats['L_ablated']['val']:.4f}\")\n",
        "            print(f\"train attention ablation mask density {stats['attention_ablation_mask_density']['train']:.4f}, val {stats['attention_ablation_mask_density']['val']:.4f}\")\n",
        "            print(f\"train neuron ablation mask density {stats['neuron_ablation_mask_density']['train']:.4f}, val {stats['neuron_ablation_mask_density']['val']:.4f}\")\n",
        "            print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.4f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if stats['loss']['val'] < best_val_loss:\n",
        "                best_val_loss = stats['loss']['val']\n",
        "                torch.save(model.state_dict(), config.save_path)\n",
        "                print(f\"New best model saved to {config.save_path}\")\n",
        "\n",
        "    print(\"Training completed!\")"
      ],
      "metadata": {
        "id": "YdL9wO7fjeYr"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "_PO8I5v5kc5m"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTNeoWithSelfAblationConfig(hidden_size=128, max_position_embeddings=256)\n",
        "model = GPTNeoWithSelfAblation(config)\n",
        "\n",
        "training_config = TrainingConfig()\n",
        "\n",
        "train_gptneo(model, training_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "51146eb987d14cd0bac9301e26b3beae",
            "7abc24cd773b402f9674adac8c4cca2e",
            "a4f2456701954c89afc38734b77acff0",
            "378959849a6d48c6ae296e8bebeb42e1",
            "93e5fa59aebb4c7a865722c5cb723b33",
            "9ec7a5394b934bfd8a9b146583af080a",
            "5c855c92fa7042359563034e805d2f95",
            "549860d6071c482ea43b8f1daab810f3",
            "dd5e50580c0b4c63b8e41f263765c9a0",
            "3669c65d683a46458989e237b0e4dba4",
            "3bd12208a06448d4b13f436d667660b2"
          ]
        },
        "id": "SN-O8jt1jho1",
        "outputId": "93e3fb90-4086-4369-be88-2c79d2bf28a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/120000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51146eb987d14cd0bac9301e26b3beae"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fSICjxhgjt3U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}