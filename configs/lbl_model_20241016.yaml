_wandb:
    value:
        cli_version: 0.18.0
        m: []
        python_version: 3.11.9
        t:
            "1":
                - 1
                - 11
                - 49
                - 51
                - 55
                - 71
            "2":
                - 1
                - 11
                - 49
                - 51
                - 55
                - 71
            "3":
                - 2
                - 3
                - 16
                - 23
                - 55
            "4": 3.11.9
            "5": 0.18.0
            "6": 4.44.2
            "8":
                - 1
                - 5
            "12": 0.18.0
            "13": linux-x86_64
ablation_mask_level:
    value: layer-by-layer
ablation_processing:
    value: hard-top-K-with-soft-gradient
attention_layers:
    value:
        - global
        - global
        - global
        - global
        - global
        - global
        - global
        - global
batch_size:
    value: 24
block_size:
    value: 256
dataset_name:
    value: TinyStories
device:
    value: cuda
eval_iters:
    value: 100
hidden_size:
    value: 128
k_attention:
    value: 2
k_neurons:
    value: 2
learning_rate:
    value: 0.0014
log_interval:
    value: 1000
loss_coeff_ablated:
    value: 0.5
loss_coeff_base:
    value: 0.5
lr_schedule:
    value: CosineAnnealing
max_grad_norm:
    value: 1
max_position_embeddings:
    value: 256
mlp_hidden_size:
    value: 512
num_batches:
    value: 400000
num_heads:
    value: 16
num_layers:
    value: 8
per_layer_ablation_position:
    value: pre
reconstruction_coeff:
    value: 0
reconstuction_loss:
    value: null
save_path:
    value: best_model_20241016.pt
temperature_attention:
    value: 1
temperature_neurons:
    value: 1
top_k_epsilon:
    value: 1e-12
top_k_level:
    value: layer-by-layer
train_file:
    value: train.bin
val_file:
    value: validation.bin
vocab_size:
    value: 50257
weight_decay:
    value: 0
window_size:
    value: 256
